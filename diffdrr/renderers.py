# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/api/01_renderers.ipynb.

# %% auto 0
__all__ = ['Siddon', 'Trilinear']

# %% ../notebooks/api/01_renderers.ipynb 3
import torch

# %% ../notebooks/api/01_renderers.ipynb 7
class Siddon(torch.nn.Module):
    """Differentiable X-ray renderer implemented with Siddon's method for exact raytracing."""

    def __init__(self, eps=1e-8):
        super().__init__()
        self.eps = eps

    def dims(self, volume):
        return torch.tensor(volume.shape).to(volume) + 1

    def maxidx(self, volume):
        return volume.numel() - 1

    def forward(self, volume, origin, spacing, source, target, mask=None):
        dims = self.dims(volume)
        maxidx = self.maxidx(volume)
        origin = origin.to(torch.float64)

        alphas = _get_alphas(source, target, origin, spacing, dims, self.eps)
        alphamid = (alphas[..., 0:-1] + alphas[..., 1:]) / 2
        voxels, idxs = _get_voxel(
            alphamid, source, target, volume, origin, spacing, dims, maxidx, self.eps
        )

        # Step length for alphas out of range will be nan
        # These nans cancel out voxels convereted to 0 index
        step_length = torch.diff(alphas, dim=-1)
        weighted_voxels = voxels * step_length

        # Handle optional masking
        if mask is None:
            img = torch.nansum(weighted_voxels, dim=-1)
            img = img.unsqueeze(1)
        else:
            # Thanks to @Ivan for the clutch assist w/ pytorch tensor ops
            # https://stackoverflow.com/questions/78323859/broadcast-pytorch-array-across-channels-based-on-another-array/78324614#78324614
            channels = torch.take(mask, idxs)  # B D N
            weighted_voxels = weighted_voxels.nan_to_num()
            B, D, N = weighted_voxels.shape
            C = mask.max().item() + 1
            img = (
                torch.zeros(B, C, D)
                .to(volume)
                .scatter_add_(
                    1, channels.transpose(-1, -2), weighted_voxels.transpose(-1, -2)
                )
            )

        # Finish rendering the DRR
        raylength = (target - source + self.eps).norm(dim=-1)
        img *= raylength.unsqueeze(1)
        return img

# %% ../notebooks/api/01_renderers.ipynb 8
def _get_alphas(source, target, origin, spacing, dims, eps):
    # Get the CT sizing and spacing parameters
    alphax = torch.arange(dims[0]).to(source) * spacing[0] + origin[0]
    alphay = torch.arange(dims[1]).to(source) * spacing[1] + origin[1]
    alphaz = torch.arange(dims[2]).to(source) * spacing[2] + origin[2]

    # Get the alpha at each plane intersection
    sx, sy, sz = source[..., 0], source[..., 1], source[..., 2]
    alphax = alphax.expand(len(source), 1, -1) - sx.unsqueeze(-1)
    alphay = alphay.expand(len(source), 1, -1) - sy.unsqueeze(-1)
    alphaz = alphaz.expand(len(source), 1, -1) - sz.unsqueeze(-1)

    sdd = target - source + eps
    alphax = alphax / sdd[..., 0].unsqueeze(-1)
    alphay = alphay / sdd[..., 1].unsqueeze(-1)
    alphaz = alphaz / sdd[..., 2].unsqueeze(-1)
    alphas = torch.cat([alphax, alphay, alphaz], dim=-1)

    # Get the alphas within the range [alphamin, alphamax]
    alphamin, alphamax = _get_alpha_minmax(sdd, source, target, origin, spacing, dims)
    good_idxs = torch.logical_and(alphas >= alphamin, alphas <= alphamax)
    alphas[~good_idxs] = torch.nan

    # Sort the alphas by ray, putting nans at the end of the list
    alphas = torch.sort(alphas, dim=-1).values

    # Drop indices where alphas for all rays are nan
    alphas = alphas[..., ~alphas.isnan().all(dim=0).all(dim=0)]

    return alphas


def _get_alpha_minmax(sdd, source, target, origin, spacing, dims):
    planes = torch.zeros(3).to(source)
    alpha0 = (planes * spacing + origin - source) / sdd
    planes = (dims - 1).to(source)
    alpha1 = (planes * spacing + origin - source) / sdd
    alphas = torch.stack([alpha0, alpha1]).to(source)

    alphamin = alphas.min(dim=0).values.max(dim=-1).values.unsqueeze(-1)
    alphamax = alphas.max(dim=0).values.min(dim=-1).values.unsqueeze(-1)

    alphamin = torch.where(alphamin < 0.0, 0.0, alphamin)
    alphamax = torch.where(alphamax > 1.0, 1.0, alphamax)
    return alphamin, alphamax


def _get_voxel(alpha, source, target, volume, origin, spacing, dims, maxidx, eps):
    idxs = _get_index(alpha, source, target, origin, spacing, dims, maxidx, eps)
    return torch.take(volume, idxs), idxs


def _get_index(alpha, source, target, origin, spacing, dims, maxidx, eps):
    sdd = target - source + eps
    idxs = source.unsqueeze(2) + alpha.unsqueeze(-1) * sdd.unsqueeze(2)
    idxs = (idxs - origin) / spacing
    idxs = idxs.floor()
    # Conversion to long makes nan->-inf, so temporarily replace them with 0
    # This is cancelled out later by multiplication by nan step_length
    idxs = (
        idxs[..., 0] * (dims[1] - 1) * (dims[2] - 1)
        + idxs[..., 1] * (dims[2] - 1)
        + idxs[..., 2]
    ).long()
    idxs[idxs < 0] = 0
    idxs[idxs > maxidx] = maxidx
    return idxs

# %% ../notebooks/api/01_renderers.ipynb 10
from torch.nn.functional import grid_sample


class Trilinear(torch.nn.Module):
    """Differentiable X-ray renderer implemented with trilinear interpolation."""

    def __init__(
        self,
        near=0.0,
        far=1.0,
        mode="bilinear",
        eps=1e-8,
    ):
        super().__init__()
        self.near = near
        self.far = far
        self.mode = mode
        self.eps = eps

    def dims(self, volume):
        return torch.tensor(volume.shape).to(volume) - 1

    def forward(
        self,
        volume,
        origin,
        spacing,
        source,
        target,
        n_points=500,
        align_corners=True,
        mask=None,
    ):
        # Get the raylength and reshape sources
        raylength = (source - target + self.eps).norm(dim=-1).unsqueeze(1)
        source = source[:, None, :, None, :] - origin
        target = target[:, None, :, None, :] - origin

        # Sample points along the rays and rescale to [-1, 1]
        alphas = torch.linspace(self.near, self.far, n_points).to(volume)
        alphas = alphas[None, None, None, :, None]
        rays = source + alphas * (target - source)
        rays = 2 * rays / (spacing * self.dims(volume)) - 1

        # Reorder array to match torch conventions
        volume = volume.permute(2, 1, 0)
        if mask is not None:
            mask = mask.permute(2, 1, 0)

        # Render the DRR
        batch_size = len(rays)
        img = grid_sample(
            volume[None, None, :, :, :].expand(batch_size, -1, -1, -1, -1),
            rays,
            mode=self.mode,
            align_corners=align_corners,
        )[:, 0, 0]

        # Handle optional masking
        if mask is None:
            img = img.sum(dim=-1).unsqueeze(1)
        else:
            B, D, N = img.shape
            C = mask.max().item() + 1
            channels = grid_sample(
                mask[None, None, :, :, :].expand(batch_size, -1, -1, -1, -1).float(),
                rays,
                mode="nearest",
                align_corners=align_corners,
            ).long()[:, 0, 0]
            img = (
                torch.zeros(B, C, D)
                .to(volume)
                .scatter_add_(1, channels.transpose(-1, -2), img.transpose(-1, -2))
            )

        # Multiply by raylength
        img *= raylength / n_points
        return img
