{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f53988ba",
   "metadata": {},
   "source": [
    "---\n",
    "title: visualization\n",
    "description: Visualize and animate DRRs in 2D and 3D\n",
    "output-file: visualization.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eedf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101fb02-aa5f-476c-bf49-c9750b181e89",
   "metadata": {},
   "source": [
    "## 2D Visualization\n",
    "\n",
    "Uses `matplotlib` and `imageio` to plot DRRs and animate optimization over DRRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_drr(\n",
    "    img: torch.Tensor,\n",
    "    title: str | None = None,\n",
    "    ticks: bool | None = True,\n",
    "    axs: matplotlib.axes._axes.Axes | None = None,\n",
    "    cmap: str = \"gray\",\n",
    "    **imshow_kwargs,\n",
    "):\n",
    "    \"\"\"Plot an image generated by a DRR module.\"\"\"\n",
    "\n",
    "    n_imgs = len(img)\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(ncols=n_imgs, figsize=(10, 5))\n",
    "    if n_imgs == 1:\n",
    "        axs = [axs]\n",
    "    if title is None or isinstance(title, str):\n",
    "        title = [title] * n_imgs\n",
    "    for img, ax, title in zip(img, axs, title):\n",
    "        ax.imshow(img.squeeze().cpu().detach(), cmap=cmap, **imshow_kwargs)\n",
    "        _, height, width = img.shape\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set(\n",
    "            xlabel=title,\n",
    "            xticks=[0, width - 1],\n",
    "            xticklabels=[1, width],\n",
    "            yticks=[0, height - 1],\n",
    "            yticklabels=[1, height],\n",
    "        )\n",
    "        if ticks is False:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999c1ac-0aa0-4f8e-821e-431f6dfb6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_mask(\n",
    "    img: torch.Tensor,\n",
    "    axs: matplotlib.axes._axes.Axes,\n",
    "    colors=[\n",
    "        \"rgb(102,194,165)\",\n",
    "        \"rgb(252,141,98)\",\n",
    "        \"rgb(141,160,203)\",\n",
    "        \"rgb(231,138,195)\",\n",
    "        \"rgb(166,216,84)\",\n",
    "        \"rgb(255,217,47)\",\n",
    "        \"rgb(229,196,148)\",\n",
    "    ],\n",
    "    alpha=0.625,\n",
    "    return_masks=False,\n",
    "):\n",
    "    \"\"\"Plot a 2D rendered segmentation mask. Meant to be called after plot_drr.\"\"\"\n",
    "\n",
    "    if len(img) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    colors = [[int(c) for c in color[4:][:-1].split(\",\")] for color in colors]\n",
    "    masks = (img > 0).unsqueeze(-1).expand(-1, -1, -1, -1, 4)\n",
    "    masks = masks.to(torch.uint8).cpu().detach()\n",
    "    masks[..., 3] *= 255\n",
    "    for idx, color in enumerate(colors):\n",
    "        masks[:, idx :: len(colors), ..., :3] *= torch.tensor(color)\n",
    "\n",
    "    for idx, ax in enumerate(axs):\n",
    "        for jdx in range(masks.shape[1]):\n",
    "            ax.imshow(masks[idx, jdx], alpha=alpha)\n",
    "\n",
    "    if return_masks:\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pathlib\n",
    "\n",
    "import pandas\n",
    "\n",
    "from diffdrr.drr import DRR\n",
    "\n",
    "\n",
    "def animate(\n",
    "    out: str | pathlib.Path,  # Savepath\n",
    "    df: pandas.DataFrame,\n",
    "    drr: DRR,\n",
    "    parameterization: str,\n",
    "    convention: str = None,\n",
    "    ground_truth: torch.Tensor | None = None,\n",
    "    verbose: bool = True,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\",\n",
    "    **kwargs,  # To pass to imageio.v3.imwrite\n",
    "):\n",
    "    \"\"\"Animate the optimization of a DRR.\"\"\"\n",
    "    # Make the axes\n",
    "    if ground_truth is None:\n",
    "\n",
    "        def make_fig():\n",
    "            fig, ax_opt = plt.subplots(\n",
    "                figsize=(3, 3),\n",
    "                constrained_layout=True,\n",
    "            )\n",
    "            return fig, ax_opt\n",
    "\n",
    "    else:\n",
    "\n",
    "        def make_fig(ground_truth):\n",
    "            fig, (ax_fix, ax_opt) = plt.subplots(\n",
    "                ncols=2,\n",
    "                figsize=(6, 3),\n",
    "                constrained_layout=True,\n",
    "            )\n",
    "            plot_drr(ground_truth, axs=ax_fix)\n",
    "            ax_fix.set(xlabel=\"Fixed DRR\")\n",
    "            return fig, ax_opt\n",
    "\n",
    "    # Compute DRRs, plot, and save to temporary folder\n",
    "    if verbose:\n",
    "        itr = tqdm(df.iterrows(), desc=\"Precomputing DRRs\", total=len(df), ncols=75)\n",
    "    else:\n",
    "        itr = df.iterrows()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        idxs = []\n",
    "        for idx, row in itr:\n",
    "            fig, ax_opt = make_fig() if ground_truth is None else make_fig(ground_truth)\n",
    "            params = row[[\"alpha\", \"beta\", \"gamma\", \"bx\", \"by\", \"bz\"]].values\n",
    "            rotations = (\n",
    "                torch.tensor(row[[\"alpha\", \"beta\", \"gamma\"]].values)\n",
    "                .unsqueeze(0)\n",
    "                .to(device=device, dtype=dtype)\n",
    "            )\n",
    "            translations = (\n",
    "                torch.tensor(row[[\"bx\", \"by\", \"bz\"]].values)\n",
    "                .unsqueeze(0)\n",
    "                .to(device=device, dtype=dtype)\n",
    "            )\n",
    "            itr = drr(\n",
    "                rotations,\n",
    "                translations,\n",
    "                parameterization=parameterization,\n",
    "                convention=convention,\n",
    "            )\n",
    "            _ = plot_drr(itr, axs=ax_opt)\n",
    "            ax_opt.set(xlabel=f\"Moving DRR (loss = {row['loss']:.3f})\")\n",
    "            fig.savefig(f\"{tmpdir}/{idx}.png\")\n",
    "            plt.close(fig)\n",
    "            idxs.append(idx)\n",
    "        frames = np.stack(\n",
    "            [iio.imread(f\"{tmpdir}/{idx}.png\") for idx in idxs],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    # Make the animation\n",
    "    return iio.imwrite(out, frames, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccffa4-7d85-44ab-b620-5f584718ebed",
   "metadata": {},
   "source": [
    "`df` is a `pandas.DataFrame` with columns `[\"alpha\", \"beta\", \"gamma\", \"bx\", \"by\", \"bz\"]`. Each row in `df` is an iteration of optimization with the updated values for that timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf37a20-22a5-4b31-9411-4ad1998ca345",
   "metadata": {},
   "source": [
    "## 3D Visualization\n",
    "\n",
    "Uses `pyvista` and `trame` to interactively visualize DRR geometry in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6718de4-26c6-4550-9d88-780fa5b809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pyvista\n",
    "import vtk\n",
    "from torchio import Subject\n",
    "\n",
    "vtk.vtkLogger.SetStderrVerbosity(vtk.vtkLogger.ConvertToVerbosity(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bce480-3f93-49ba-bdd5-8677ac1e8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def drr_to_mesh(\n",
    "    subject: Subject,  # torchio.Subject with a `volume` attribute\n",
    "    method: str,  # Either `surface_nets` or `marching_cubes`\n",
    "    threshold: float = 150,  # Min value for marching cubes (Hounsfield units)\n",
    "    extract_largest: bool = True,  # Extract the largest connected component from the mesh\n",
    "    verbose: bool = True,  # Display progress bars for mesh processing steps\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert the CT in a DRR object into a mesh.\n",
    "\n",
    "    If using `method==\"surface_nets\"`, ensure you have `pyvista>=0.43` and `vtk>=9.3` installed.\n",
    "\n",
    "    The mesh processing steps are:\n",
    "\n",
    "    1. Keep only largest connected components (optional)\n",
    "    2. Smooth\n",
    "    3. Decimate (if `method==\"marching_cubes\"`)\n",
    "    4. Fill any holes\n",
    "    5. Clean (remove any redundant vertices/edges)\n",
    "    \"\"\"\n",
    "    # Turn the CT into a PyVista object\n",
    "    grid = pyvista.ImageData(\n",
    "        dimensions=subject.volume.spatial_shape, spacing=(1, 1, 1), origin=(0, 0, 0)\n",
    "    )\n",
    "\n",
    "    # Run surface extraction\n",
    "    if method == \"marching_cubes\":\n",
    "        mesh = grid.contour(\n",
    "            isosurfaces=1,\n",
    "            scalars=subject.volume.data[0].cpu().numpy().flatten(order=\"F\"),\n",
    "            rng=[threshold, torch.inf],\n",
    "            method=\"marching_cubes\",\n",
    "            progress_bar=verbose,\n",
    "        )\n",
    "    elif method == \"surface_nets\":\n",
    "        grid.point_data[\"values\"] = (\n",
    "            subject.volume.data[0].cpu().numpy().flatten(order=\"F\") > threshold\n",
    "        )\n",
    "        try:\n",
    "            mesh = grid.contour_labeled(smoothing=True, progress_bar=verbose)\n",
    "        except AttributeError as e:\n",
    "            raise AttributeError(\n",
    "                f\"{e}, ensure you are using pyvista>=0.43 and vtk>=9.3\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"method must be `marching_cubes` or `surface_nets`, not {method}\"\n",
    "        )\n",
    "\n",
    "    # Transform the mesh using the affine matrix\n",
    "    mesh = mesh.transform(subject.volume.affine.squeeze())\n",
    "\n",
    "    # Preprocess the mesh\n",
    "    if extract_largest:\n",
    "        mesh.extract_largest(inplace=True, progress_bar=verbose)\n",
    "    mesh.point_data.clear()\n",
    "    mesh.cell_data.clear()\n",
    "    mesh.smooth_taubin(\n",
    "        n_iter=100,\n",
    "        feature_angle=120.0,\n",
    "        boundary_smoothing=False,\n",
    "        feature_smoothing=False,\n",
    "        non_manifold_smoothing=True,\n",
    "        normalize_coordinates=True,\n",
    "        inplace=True,\n",
    "        progress_bar=verbose,\n",
    "    )\n",
    "    if method == \"marching_cubes\":\n",
    "        mesh.decimate_pro(0.25, inplace=True, progress_bar=verbose)\n",
    "    mesh.fill_holes(100, inplace=True, progress_bar=verbose)\n",
    "    mesh.clean(inplace=True, progress_bar=verbose)\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422595b9-3f28-4c0e-b549-837c8d87430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def labelmap_to_mesh(\n",
    "    subject: Subject,  # torchio.Subject with  a `mask` attribute\n",
    "    verbose: bool = True,  # Display progress bars for mesh processing steps\n",
    "):\n",
    "    # Turn the 3D labelmap into a PyVista object\n",
    "    grid = pyvista.ImageData(\n",
    "        dimensions=subject.mask.spatial_shape, spacing=(1, 1, 1), origin=(0, 0, 0)\n",
    "    )\n",
    "\n",
    "    # Run SurfaceNets\n",
    "    grid.point_data[\"values\"] = subject.mask.data[0].numpy().flatten(order=\"F\")\n",
    "    mesh = grid.contour_labeled(smoothing=True, progress_bar=verbose)\n",
    "    mesh.smooth_taubin(\n",
    "        n_iter=100,\n",
    "        feature_angle=120.0,\n",
    "        boundary_smoothing=False,\n",
    "        feature_smoothing=False,\n",
    "        non_manifold_smoothing=True,\n",
    "        normalize_coordinates=True,\n",
    "        inplace=True,\n",
    "        progress_bar=verbose,\n",
    "    )\n",
    "    mesh.clean(inplace=True, progress_bar=verbose)\n",
    "\n",
    "    # Transform the mesh using the affine matrix\n",
    "    mesh = mesh.transform(subject.mask.affine.squeeze())\n",
    "\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bec80c-947e-4b66-8205-c540a1342f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from diffdrr.pose import RigidTransform\n",
    "\n",
    "\n",
    "def img_to_mesh(\n",
    "    drr: DRR, pose: RigidTransform, calibration: RigidTransform = None, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given pose (not batched), turn the camera and detector into a mesh.\n",
    "    Additionally, render the DRR for the pose. Convert into a texture that\n",
    "    can be applied to the detector mesh.\n",
    "    \"\"\"\n",
    "    # Turn DRR img into a texture that can be applied to a mesh\n",
    "    img = drr(pose, calibration)\n",
    "    img = img.cpu().squeeze().detach().numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    img = (255.0 * img).astype(np.uint8)\n",
    "    texture = pyvista.numpy_to_texture(img)\n",
    "\n",
    "    # Make a mesh for the camera and the principal ray\n",
    "    source, target = drr.detector(pose, calibration)\n",
    "    source = source.squeeze().cpu().detach().numpy()\n",
    "    target = (\n",
    "        target.reshape(drr.detector.height, drr.detector.width, 3)\n",
    "        .cpu()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    principal_ray = pyvista.Line(source, target.mean(axis=0).mean(axis=0))\n",
    "    camera = _make_camera_frustum_mesh(source, target, size=0.125)\n",
    "\n",
    "    # Make a mesh for the detector plane\n",
    "    detector = pyvista.StructuredGrid(\n",
    "        target[..., 0],\n",
    "        target[..., 1],\n",
    "        target[..., 2],\n",
    "    )\n",
    "    detector.add_field_data([drr.detector.height], \"height\")\n",
    "    detector.add_field_data([drr.detector.width], \"width\")\n",
    "    detector.texture_map_to_plane(\n",
    "        origin=target[-1, 0],\n",
    "        point_u=target[-1, -1],\n",
    "        point_v=target[0, 0],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return camera, detector, texture, principal_ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0de96-4efa-46ff-a337-3771c9a343e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _make_camera_frustum_mesh(source, target, size=0.125):\n",
    "    vertices = np.stack(\n",
    "        [\n",
    "            source + size * (target[0, 0] - source),\n",
    "            source + size * (target[-1, 0] - source),\n",
    "            source + size * (target[-1, -1] - source),\n",
    "            source + size * (target[0, -1] - source),\n",
    "            source,\n",
    "        ]\n",
    "    )\n",
    "    faces = np.hstack(\n",
    "        [\n",
    "            [4, 0, 1, 2, 3],\n",
    "            [3, 0, 1, 4],\n",
    "            [3, 1, 2, 4],\n",
    "            [3, 0, 3, 4],\n",
    "            [3, 2, 3, 4],\n",
    "        ]\n",
    "    )\n",
    "    return pyvista.PolyData(vertices, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253e372-c4da-416d-ad50-c561369ca9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
